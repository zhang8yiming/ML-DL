{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhang8yiming/ML-DL/blob/main/L7_Homework_Reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFSOWJPpt0tX"
      },
      "source": [
        "\n",
        "# 用不同的优化方法训练验证码识别模型\n",
        "\n",
        "## 作业内容\n",
        "\n",
        "在本次作业中，你将：\n",
        "- 用不同的优化方法训练验证码识别模型；\n",
        "- 调整学习率提高模型精度；\n",
        "- 查看神经网络识别验证码字母的效果。\n",
        "\n",
        "## 数据集介绍\n",
        "\n",
        "数据集是验证码图像，图像是 **5 个字母** 的单词，并且含有噪点（模糊和线条）。 它们的尺寸为 **200 x 50**。 文件名与图像字母相同。数据集来源 [kaggle](https://www.kaggle.com/fournierp/captcha-version-2-images)."
      ],
      "id": "kFSOWJPpt0tX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:12.137068Z",
          "iopub.status.busy": "2022-09-12T13:31:12.136559Z",
          "iopub.status.idle": "2022-09-12T13:31:13.062964Z",
          "shell.execute_reply": "2022-09-12T13:31:13.062558Z",
          "shell.execute_reply.started": "2022-09-12T13:31:12.136934Z"
        },
        "tags": [],
        "id": "3E_bCA5Pt0tc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable"
      ],
      "id": "3E_bCA5Pt0tc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:13.064052Z",
          "iopub.status.busy": "2022-09-12T13:31:13.063860Z",
          "iopub.status.idle": "2022-09-12T13:31:13.067284Z",
          "shell.execute_reply": "2022-09-12T13:31:13.066883Z",
          "shell.execute_reply.started": "2022-09-12T13:31:13.064034Z"
        },
        "tags": [],
        "id": "UPs4ezVRt0td"
      },
      "outputs": [],
      "source": [
        "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "ALL_CHAR_SET = NUMBER + ALPHABET\n",
        "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
        "MAX_CAPTCHA = 5"
      ],
      "id": "UPs4ezVRt0td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:13.068114Z",
          "iopub.status.busy": "2022-09-12T13:31:13.067934Z",
          "iopub.status.idle": "2022-09-12T13:31:13.137178Z",
          "shell.execute_reply": "2022-09-12T13:31:13.136203Z",
          "shell.execute_reply.started": "2022-09-12T13:31:13.068095Z"
        },
        "tags": [],
        "id": "0VMKrME6t0te"
      },
      "outputs": [],
      "source": [
        "def encode(a):\n",
        "    onehot = [0]*ALL_CHAR_SET_LEN\n",
        "    idx = ALL_CHAR_SET.index(a)\n",
        "    onehot[idx] += 1\n",
        "    return onehot\n",
        "\n",
        "class Mydataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.path[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img = img.convert('L')\n",
        "        label = img_path[-9:-4]\n",
        "        # print(img_path, label)\n",
        "        label_oh = []\n",
        "        for i in label:\n",
        "            label_oh += encode(i)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, np.array(label_oh), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.path)"
      ],
      "id": "0VMKrME6t0te"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:13.140137Z",
          "iopub.status.busy": "2022-09-12T13:31:13.139667Z",
          "iopub.status.idle": "2022-09-12T13:31:13.197736Z",
          "shell.execute_reply": "2022-09-12T13:31:13.196760Z",
          "shell.execute_reply.started": "2022-09-12T13:31:13.140088Z"
        },
        "tags": [],
        "id": "xu4EYhsIt0tf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize([224, 224]),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "id": "xu4EYhsIt0tf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:13.200167Z",
          "iopub.status.busy": "2022-09-12T13:31:13.199710Z",
          "iopub.status.idle": "2022-09-12T13:31:13.260374Z",
          "shell.execute_reply": "2022-09-12T13:31:13.259239Z",
          "shell.execute_reply.started": "2022-09-12T13:31:13.200119Z"
        },
        "tags": [],
        "id": "4SCiWRRrt0tg"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "path = './captcha_images_v2/*.png'"
      ],
      "id": "4SCiWRRrt0tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:14.139775Z",
          "iopub.status.busy": "2022-09-12T13:31:14.138869Z",
          "iopub.status.idle": "2022-09-12T13:31:14.152956Z",
          "shell.execute_reply": "2022-09-12T13:31:14.152279Z",
          "shell.execute_reply.started": "2022-09-12T13:31:14.139722Z"
        },
        "tags": [],
        "id": "7UgG6UHZt0tg"
      },
      "outputs": [],
      "source": [
        "train_ds = Mydataset(glob.glob(path)[:-100], transform=transform)\n",
        "test_ds = Mydataset(glob.glob(path)[-100:], transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, num_workers=0)\n",
        "test_dl = DataLoader(train_ds, batch_size=1, num_workers=0)"
      ],
      "id": "7UgG6UHZt0tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:14.450511Z",
          "iopub.status.busy": "2022-09-12T13:31:14.450170Z",
          "iopub.status.idle": "2022-09-12T13:31:14.580244Z",
          "shell.execute_reply": "2022-09-12T13:31:14.579696Z",
          "shell.execute_reply.started": "2022-09-12T13:31:14.450474Z"
        },
        "tags": [],
        "id": "fdylPLGrt0th",
        "outputId": "96917f70-2d5b-4125-99dc-fdbd886684cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.7529, 0.7529, 0.7529,  ..., 0.9843, 0.9843, 0.9843],\n",
              "          [0.7529, 0.7529, 0.7529,  ..., 0.9843, 0.9843, 0.9843],\n",
              "          [0.7529, 0.7529, 0.7529,  ..., 0.9843, 0.9843, 0.9843],\n",
              "          ...,\n",
              "          [0.7647, 0.7647, 0.7647,  ..., 0.9961, 0.9961, 0.9961],\n",
              "          [0.7647, 0.7647, 0.7647,  ..., 0.9961, 0.9961, 0.9961],\n",
              "          [0.7647, 0.7647, 0.7647,  ..., 0.9961, 0.9961, 0.9961]]]),\n",
              " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0]),\n",
              " '5nggg')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds[0]"
      ],
      "id": "fdylPLGrt0th"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:15.139147Z",
          "iopub.status.busy": "2022-09-12T13:31:15.138741Z",
          "iopub.status.idle": "2022-09-12T13:31:15.970006Z",
          "shell.execute_reply": "2022-09-12T13:31:15.969485Z",
          "shell.execute_reply.started": "2022-09-12T13:31:15.139112Z"
        },
        "tags": [],
        "id": "WRnBxYSat0ti"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)"
      ],
      "id": "WRnBxYSat0ti"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:16.139652Z",
          "iopub.status.busy": "2022-09-12T13:31:16.139042Z",
          "iopub.status.idle": "2022-09-12T13:31:16.143487Z",
          "shell.execute_reply": "2022-09-12T13:31:16.143006Z",
          "shell.execute_reply.started": "2022-09-12T13:31:16.139599Z"
        },
        "tags": [],
        "id": "Sc9eBYIYt0ti"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "id": "Sc9eBYIYt0ti"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:31:17.139423Z",
          "iopub.status.busy": "2022-09-12T13:31:17.138827Z",
          "iopub.status.idle": "2022-09-12T13:32:21.674184Z",
          "shell.execute_reply": "2022-09-12T13:32:21.673754Z",
          "shell.execute_reply.started": "2022-09-12T13:31:17.139371Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "ntLwkvMLt0tj",
        "outputId": "c3073409-751c-47dc-bd7a-07b6119dad39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eopch: 1 step: 1 loss: 0.7659690380096436\n",
            "eopch: 1 step: 2 loss: 0.5586223006248474\n",
            "eopch: 1 step: 3 loss: 0.3903140723705292\n",
            "eopch: 1 step: 4 loss: 0.2698208689689636\n",
            "eopch: 1 step: 5 loss: 0.19641001522541046\n",
            "eopch: 1 step: 6 loss: 0.15496386587619781\n",
            "eopch: 1 step: 7 loss: 0.13451442122459412\n",
            "eopch: 1 step: 8 loss: 0.12765008211135864\n",
            "eopch: 1 step: 9 loss: 0.12417804449796677\n",
            "eopch: 1 step: 10 loss: 0.12543776631355286\n",
            "eopch: 1 step: 11 loss: 0.12637782096862793\n",
            "eopch: 1 step: 12 loss: 0.12565430998802185\n",
            "eopch: 1 step: 13 loss: 0.1258484423160553\n",
            "eopch: 1 step: 14 loss: 0.1262722909450531\n",
            "eopch: 1 step: 15 loss: 0.12370920926332474\n",
            "eopch: 2 step: 1 loss: 0.1229570060968399\n",
            "eopch: 2 step: 2 loss: 0.12030627578496933\n",
            "eopch: 2 step: 3 loss: 0.12015184760093689\n",
            "eopch: 2 step: 4 loss: 0.11753649264574051\n",
            "eopch: 2 step: 5 loss: 0.11459614336490631\n",
            "eopch: 2 step: 6 loss: 0.11422881484031677\n",
            "eopch: 2 step: 7 loss: 0.11289031058549881\n",
            "eopch: 2 step: 8 loss: 0.11080260574817657\n",
            "eopch: 2 step: 9 loss: 0.11027084290981293\n",
            "eopch: 2 step: 10 loss: 0.10829078406095505\n",
            "eopch: 2 step: 11 loss: 0.10802160203456879\n",
            "eopch: 2 step: 12 loss: 0.10648937523365021\n",
            "eopch: 2 step: 13 loss: 0.1053670346736908\n",
            "eopch: 2 step: 14 loss: 0.1046212688088417\n",
            "eopch: 2 step: 15 loss: 0.10235761851072311\n",
            "eopch: 3 step: 1 loss: 0.10206548124551773\n",
            "eopch: 3 step: 2 loss: 0.0995779037475586\n",
            "eopch: 3 step: 3 loss: 0.10030505806207657\n",
            "eopch: 3 step: 4 loss: 0.0988355502486229\n",
            "eopch: 3 step: 5 loss: 0.0967763364315033\n",
            "eopch: 3 step: 6 loss: 0.09584429860115051\n",
            "eopch: 3 step: 7 loss: 0.09540801495313644\n",
            "eopch: 3 step: 8 loss: 0.09392642974853516\n",
            "eopch: 3 step: 9 loss: 0.09263992309570312\n",
            "eopch: 3 step: 10 loss: 0.09190364927053452\n",
            "eopch: 3 step: 11 loss: 0.09134797751903534\n",
            "eopch: 3 step: 12 loss: 0.0895373672246933\n",
            "eopch: 3 step: 13 loss: 0.08813484758138657\n",
            "eopch: 3 step: 14 loss: 0.08770516514778137\n",
            "eopch: 3 step: 15 loss: 0.08408184349536896\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(3):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "        print('eopch:', epoch+1, 'step:', step+1, 'loss:', loss.item())"
      ],
      "id": "ntLwkvMLt0tj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:33:24.330460Z",
          "iopub.status.busy": "2022-09-12T13:33:24.329848Z",
          "iopub.status.idle": "2022-09-12T13:33:24.363965Z",
          "shell.execute_reply": "2022-09-12T13:33:24.363430Z",
          "shell.execute_reply.started": "2022-09-12T13:33:24.330406Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "OSr67s8Ft0tj",
        "outputId": "baa7c91d-645e-4b2c-9f88-0d31e58997c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: 5nggg pred: 37pgg\n"
          ]
        }
      ],
      "source": [
        "model.eval();\n",
        "\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    print('label:', label[0], 'pred:', c)\n",
        "    \n",
        "    break"
      ],
      "id": "OSr67s8Ft0tj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWmwnjPat0tk"
      },
      "source": [
        "## 问题1：准确率计算\n",
        "\n",
        "在上述验证集的代码中并没有包含准确率计算逻辑，请加入以下代码：\n",
        "- 字符准确率\n",
        "- 整张图片预测准确率"
      ],
      "id": "DWmwnjPat0tk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:37:11.357888Z",
          "iopub.status.busy": "2022-09-12T13:37:11.356841Z",
          "iopub.status.idle": "2022-09-12T13:37:23.283031Z",
          "shell.execute_reply": "2022-09-12T13:37:23.282595Z",
          "shell.execute_reply.started": "2022-09-12T13:37:11.357809Z"
        },
        "tags": [],
        "id": "JaSwv70at0tk"
      },
      "outputs": [],
      "source": [
        "model.eval();\n",
        "\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)"
      ],
      "id": "JaSwv70at0tk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:37:26.272029Z",
          "iopub.status.busy": "2022-09-12T13:37:26.271537Z",
          "iopub.status.idle": "2022-09-12T13:37:26.277205Z",
          "shell.execute_reply": "2022-09-12T13:37:26.276706Z",
          "shell.execute_reply.started": "2022-09-12T13:37:26.271988Z"
        },
        "tags": [],
        "id": "kt7KgScUt0tl",
        "outputId": "b2cd319c-eb5f-4493-9fc5-21636ef95430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3087234042553192, 0.0)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_acc, image_acc\n",
        "# 训练时间较短，整张图片预测准确率较低正常"
      ],
      "id": "kt7KgScUt0tl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ynHtaWbt0tl"
      },
      "source": [
        "## 问题2：优化器尝试\n",
        "\n",
        "选择四种优化器，分别训练5个epoch，然后记录下各自在测试集精度。"
      ],
      "id": "1ynHtaWbt0tl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:39:01.374687Z",
          "iopub.status.busy": "2022-09-12T13:39:01.374257Z",
          "iopub.status.idle": "2022-09-12T13:39:01.379714Z",
          "shell.execute_reply": "2022-09-12T13:39:01.378683Z",
          "shell.execute_reply.started": "2022-09-12T13:39:01.374641Z"
        },
        "id": "FNRM_znvt0tl"
      },
      "source": [
        "### SGD"
      ],
      "id": "FNRM_znvt0tl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:40:29.138012Z",
          "iopub.status.busy": "2022-09-12T13:40:29.137582Z",
          "iopub.status.idle": "2022-09-12T13:42:20.712395Z",
          "shell.execute_reply": "2022-09-12T13:42:20.711843Z",
          "shell.execute_reply.started": "2022-09-12T13:40:29.137964Z"
        },
        "tags": [],
        "id": "GjqzrRjot0tl",
        "outputId": "37bd879c-e04b-41e1-e947-95dab5587ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0225531914893617 0.0\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "GjqzrRjot0tl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jjSKNoBt0tm"
      },
      "source": [
        "### AdamW"
      ],
      "id": "5jjSKNoBt0tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:43:59.137923Z",
          "iopub.status.busy": "2022-09-12T13:43:59.137318Z",
          "iopub.status.idle": "2022-09-12T13:45:55.193801Z",
          "shell.execute_reply": "2022-09-12T13:45:55.193371Z",
          "shell.execute_reply.started": "2022-09-12T13:43:59.137873Z"
        },
        "id": "k9SF2fX4t0tm",
        "outputId": "98c5ca40-02c7-4cac-bf4e-5b3caad2c4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7523404255319149 0.2351063829787234\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "k9SF2fX4t0tm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G708Hl5-t0tm"
      },
      "source": [
        "### Adadelta"
      ],
      "id": "G708Hl5-t0tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:46:17.135826Z",
          "iopub.status.busy": "2022-09-12T13:46:17.135390Z",
          "iopub.status.idle": "2022-09-12T13:48:15.077951Z",
          "shell.execute_reply": "2022-09-12T13:48:15.077532Z",
          "shell.execute_reply.started": "2022-09-12T13:46:17.135776Z"
        },
        "id": "LGW26U-bt0tm",
        "outputId": "4ad6c67b-4f28-4d66-f302-9171fdfb4af4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lyz/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/lyz/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.02191489361702128 0.0\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "LGW26U-bt0tm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KicPSnfrt0tn"
      },
      "source": [
        "### Adagrad"
      ],
      "id": "KicPSnfrt0tn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:52:44.394757Z",
          "iopub.status.busy": "2022-09-12T13:52:44.394328Z",
          "iopub.status.idle": "2022-09-12T13:54:43.968159Z",
          "shell.execute_reply": "2022-09-12T13:54:43.967744Z",
          "shell.execute_reply.started": "2022-09-12T13:52:44.394708Z"
        },
        "id": "haPJ7kKvt0tn",
        "outputId": "0673eb52-8c5b-4d9a-9311-15f195695321"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lyz/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/lyz/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6861702127659575 0.1478723404255319\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "haPJ7kKvt0tn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3W9JsMrt0tn"
      },
      "source": [
        "### RMSprop"
      ],
      "id": "V3W9JsMrt0tn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T13:57:35.136044Z",
          "iopub.status.busy": "2022-09-12T13:57:35.135495Z",
          "iopub.status.idle": "2022-09-12T13:59:40.228252Z",
          "shell.execute_reply": "2022-09-12T13:59:40.227815Z",
          "shell.execute_reply.started": "2022-09-12T13:57:35.135988Z"
        },
        "id": "gzPPUMa_t0tn"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "gzPPUMa_t0tn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEoV3SJJt0tn"
      },
      "source": [
        "## 问题3：学习率调整\n",
        "\n",
        "在选择最优的优化器中分别加入2中学习率调整策略，然后记录下各自在测试集精度。"
      ],
      "id": "zEoV3SJJt0tn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuM5ghYFt0to"
      },
      "source": [
        "### AdamW + StepLR"
      ],
      "id": "vuM5ghYFt0to"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T14:03:37.189998Z",
          "iopub.status.busy": "2022-09-12T14:03:37.189414Z",
          "iopub.status.idle": "2022-09-12T14:05:36.891941Z",
          "shell.execute_reply": "2022-09-12T14:05:36.891523Z",
          "shell.execute_reply.started": "2022-09-12T14:03:37.189947Z"
        },
        "tags": [],
        "id": "GBbCYKfwt0to",
        "outputId": "06ef45dc-92f5-49cc-b26e-766f37ee5e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8176595744680851 0.3478723404255319\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optm, step_size=50, gamma=0.7)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "        scheduler.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "GBbCYKfwt0to"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v5S_j-Zt0to"
      },
      "source": [
        "### AdamW + CosineAnnealingLR"
      ],
      "id": "0v5S_j-Zt0to"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T14:06:51.190589Z",
          "iopub.status.busy": "2022-09-12T14:06:51.190158Z",
          "iopub.status.idle": "2022-09-12T14:08:50.134796Z",
          "shell.execute_reply": "2022-09-12T14:08:50.134376Z",
          "shell.execute_reply.started": "2022-09-12T14:06:51.190538Z"
        },
        "tags": [],
        "id": "vu06BRVWt0to",
        "outputId": "43b9e610-35f8-481a-a11c-9b2a165e7784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6568085106382979 0.09574468085106383\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "optm = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optm, T_max=100, eta_min=0)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        pred = model(img)\n",
        "        loss = loss_func(pred, label_oh.float())\n",
        "        optm.zero_grad()\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "        scheduler.step()\n",
        "\n",
        "model.eval();\n",
        "char_acc = 0\n",
        "image_acc = 0\n",
        "for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "    pred = model(img)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(pred.squeeze().cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c = '%s%s%s%s%s' % (c0, c1, c2, c3, c4)\n",
        "\n",
        "    for c1, c2 in zip(label[0], c):\n",
        "        if c1 == c2:\n",
        "            char_acc += 1\n",
        "    \n",
        "    if label[0] == c:\n",
        "        image_acc += 1\n",
        "    \n",
        "char_acc = char_acc / len(test_dl.dataset) / 5\n",
        "image_acc = image_acc / len(test_dl.dataset)\n",
        "print(char_acc, image_acc)"
      ],
      "id": "vu06BRVWt0to"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppk9EYYZt0tp"
      },
      "outputs": [],
      "source": [],
      "id": "Ppk9EYYZt0tp"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}